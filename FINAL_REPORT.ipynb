{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e1755-dec6-415f-ab0d-3a83f78d1f3e",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7873555-89d0-4f2c-bc24-57f2361ebd1c",
   "metadata": {},
   "source": [
    "Humaira Halim (hbh4bv@virginia.edu) DS 5001 Spring 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400adbd2-7273-4cda-b58c-a261e63fcaa6",
   "metadata": {},
   "source": [
    "Code for report worked in collaboration with Nikita Amanna and Nicholas Kalinowski -- report and conclusions are all mine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417256f2-fc16-4bc8-aa29-32e6eb797479",
   "metadata": {},
   "source": [
    "## Introduction. \n",
    "*Describe the nature of your corpus and the question(s) you've asked of the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21b5b7-5ed7-445e-9664-5733c6e8af07",
   "metadata": {},
   "source": [
    "According to the Encyclopedia Brittanica, the genre of science fiction is defined as \"a form of fiction that deals principally with the impact of actual or imagined science upon society or individuals\" (2023). Science fiction is a relatively modern subject in written literature; forming alongside the socioeconomic changes of the Industrial Revolution, the West is accredited for its origins as writers began contemplating the consequences of technological developments. Combining text analysis with 19th-20th century Science Fiction offers an anthropological glimpse into what the visionary's imagined as the consequences of post-Industrial Revolution. \n",
    "\n",
    "For my project, I have sourced 6 popular scifi novels of the 19th-20th century from American authors. I was interested in what these authors imagined for the future of science, humored by my reality as a data scientist two centuries later. The text models used in this report to characterize my corpora are Principal Components (PCA), Topic Models (LDA), Word Embeddings (word2vec), and Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbffb8e-7b3e-42e0-a07e-fd7fa8331cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Source Data. \n",
    "*Provide a description of all relativant source files and describe the following features for each source file:*\n",
    "\n",
    "*Provenance: Where did they come from? Describe the website or other source and provide relevant URLs.*\n",
    "*Location: Provide a link to the source files in UVA Box.*\n",
    "*Description: What is the general subject matter of the corpus? How many observations are there? What is the average document length?*\n",
    "*Format: A description of both the file formats of the source files, e.g., plaintext, XML, CSV, etc., and the internal structure where applicable. For - example, if XML then specify document type (e.g., TEI or XHTML).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba4ece-4720-47df-bbaf-b0bbbd3c8934",
   "metadata": {},
   "source": [
    "The six American scifi books I have selected for analysis are: \n",
    "* *Looking Backward* (1887) by Edward Bellamy\n",
    "* *The Iron Heel* (1998) by Jack London\n",
    "* *A Voyage to the Moon* (1827) by George Tucker  \n",
    "* *The Variable Man* (1953) by Philip K Dick \n",
    "* *The Brick Moon* (1869) by Edward Everett Hale  \n",
    "* *Youth* (1952) by Isaac Asimov \n",
    "\n",
    "The novels in my corpus were all sourced from Project Gutenberg, an online archive dedicated to digitizing and preserving older works. The link to Project Guntenberg can be found [here.](https://www.gutenberg.org/) Additionally, access to the raw .txt files of my corpus via UVA box can be found [here.](https://www.dropbox.com/scl/fo/vo4jx7bw8d0ybyiy9bhdm/h?dl=0&rlkey=u49a6y1hb67ic1nnypw554ahk)\n",
    "\n",
    "Below is a basic summary chart of these observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1ba93-b6dc-4055-9a4c-90f7fd51e5a2",
   "metadata": {},
   "source": [
    "![1.png](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1dc5f-c895-423f-86aa-e89ecacb09e8",
   "metadata": {},
   "source": [
    "The average length of the books within my corpora are about 19 chapters, with a mean of 50,200 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b940d3-f6c5-4a7f-8e6a-799007961e8e",
   "metadata": {},
   "source": [
    "## Data Model. \n",
    "*Describe the analytical tables you generated in the process of tokenization, annotation, and analysis of your corpus. You provide a list of tables with field names and their definition, along with URLs to each associated CSV file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216cd0f-41e2-4828-bc9c-8787b4c73a6c",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "For the PCA, I needed to generate a bag of words and tfidf. The TFIDF table is an extension of an implied DOC table, where each doc is an observation (particularly a chapter in this case). PCA results in two tables: [loadings](https://github.com/Humairabh/DS5001FinalProject/blob/a11f204dde1c94c07d64c48649460213195c94b3/Data%20Files/american_loadings.csv) (language model) and [docs and components](https://github.com/Humairabh/DS5001FinalProject/blob/main/Data%20Files/american_components.csv) (which replaces the original document-term matrix with a reduced version). \n",
    "\n",
    "Part of the loadings table is shown below:\n",
    "![loadings.png](loadings.png)\n",
    "\n",
    "Part of the documents and components table is shown below:\n",
    "\n",
    "![dc.png](dc.png)\n",
    "\n",
    "\n",
    "#### LDA\n",
    "\n",
    "For topic models, I uses Scikit-Learn's CountVectorizer function to convert the F1 corpus into a document-term (aka DTM) vector space of word counts. From there, I could use Scikit-Learn's LatentDirichletAllocation algorithm to extract the [THETA](https://github.com/Humairabh/DS5001FinalProject/blob/a11f204dde1c94c07d64c48649460213195c94b3/Data%20Files/american_theta.csv) (doctopic where counts get normalized into PDFs) and [PHI](https://github.com/Humairabh/DS5001FinalProject/blob/a11f204dde1c94c07d64c48649460213195c94b3/Data%20Files/phi.csv) (topicword where counts get normalized into PDFs) tables. I also created a [TOPICS](https://github.com/Humairabh/DS5001FinalProject/blob/a11f204dde1c94c07d64c48649460213195c94b3/Data%20Files/topics.csv) table:\n",
    "![topic.png](topic.png)\n",
    "\n",
    "this table includes a list of the topics (0-8), most associated words by each topic, document weight and term frequency.\n",
    "\n",
    "#### Word Embeddings (word2vec)\n",
    "\n",
    "For word embeddings, it was necessary to import our TOKENS tables and generate a DOCS table for Gensim. Using Genim's word2vec function and the tsne engine, it was possible to vectorize terms into a semantic space and generate coordinates. As we've learned in lecture, the tsne function embodies a clustering method applied to high dimensional vectors, comparing pairwise similarities into probabilities. I set the parameters to vector_size=256, window=2, and min_count=50. \n",
    "\n",
    "#### Sentiment Analysis\n",
    "For sentiment analysis, I needed to generate a [vocab sentiment table](https://github.com/Humairabh/DS5001FinalProject/blob/1c407da5b92ddfa01440cea50952d98bed9d5c6d/Data%20Files/vocab_sentiment.csv). I imported the Syuzhet.csv file which provided a table of words and their \"sentiments\". Our generated table incorporates dfidf, ifidf, syu_sentiment and weighted_sentiment. \n",
    "\n",
    "![vs.png](vs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599702ac-be6e-4473-ad48-c37c5e9f17bb",
   "metadata": {},
   "source": [
    "## Exploration. \n",
    "\n",
    "*Describe each of your explorations, such as PCA and topic models. For each, include the relevant parameters and hyperparemeters used to generate each model and visualization. For your visualizations, you should use at least three (but likely more) of the following visualization types:*\n",
    "\n",
    "*Hierarchical cluster diagrams*\n",
    "\n",
    "*Heatmaps showing correlations*\n",
    "\n",
    "*Scatter plots*\n",
    "\n",
    "*KDE plots*\n",
    "\n",
    "*Dispersion plots*\n",
    "\n",
    "*t-SNE plots*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e89380-7f45-46a0-90dc-594098f4b17d",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Principle Component Analysis, or PCA, identifies and combines features with max variance. Firstly, I looked at PC0 Dispersion by book to see if there was clear differences in the spread:\n",
    "\n",
    "#### PCA Dispersion Plot\n",
    "\n",
    "![pca1.png](pca1.png)\n",
    "\n",
    "A Voyage to the Moon has a much larger range than the other books, whereas The Brick Moon and the Variable Man have a very small spread. Given outliers have a significant impact on the results of a PCA dispersion, it is suggestable that A Voyage to the Moon has a significant impact on our PCA results.\n",
    "\n",
    "Next for PCA, I wanted to look at PC0-8 to see the differences in correlation by PC:\n",
    "\n",
    "![pca2.png](pca2.png)\n",
    "\n",
    "PC0 had a much stronger correlation than the PC's 1-8 which was pretty much what I expected to see. \n",
    "\n",
    "### LDA\n",
    "\n",
    "The LDA model attempts to estimate probability distributions for topics in documents plus words in topics. LDA gives us THETA (distribution of topics over documents) and PHI (distribution of words over topics) tables. Firstly, we can see how our model grouped topics by weighting each document:\n",
    "\n",
    "#### TOPICS by Doc Weight\n",
    "![LDA1.png](LDA1.png)\n",
    "\n",
    "By looking at the doc weight of each topic, we can determine which topics are most strongly represented in the document. I noted that Topic 4 (said, ways, eyes, moment, face, house, door, room, hand) & Topic 8 (labor, class, men, nation, day, time, people, course, army) had the greatest document weight. This strong weight highlights the importance of these two topics in scifi. Now that we are able to understand the core terms of each topic, I wanted to see their term frequency by document weight:\n",
    "\n",
    "#### TOPIC Doc Weight vs Term Freq\n",
    "![LDA2.png](LDA2.png)\n",
    "\n",
    "Analyzing term frequency allowed me to estimate the likelihood of the words within my topic to be important; plotting term frequency and document weight together gives a glimpse into the most important topics and words in my corpus. While Topic 4 wasn't as important for term frequency, Topic 8 definitively excelled in both metrics.\n",
    "\n",
    "#### THETA Sample\n",
    "\n",
    "Another method of LDA is to observe theta, my table which explores the relationships between documents and topics. Here, I took a sample of 20 of my OHCO paragraphs and illustrated a heatmap with THETA values:\n",
    "\n",
    "![LDA3.png](LDA3.png)\n",
    "\n",
    "I was not surprised to see that Topic 8 had some stronger/more polar probabilities, whereas Topic 4 had a larger number of \"lukewarm\" 0.3 values than the other topics. \n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "The process of word embeddings allow us to examine meanings independent of documents. As seen in lecture, word vectors are generated by a class of unsupervised algorithms in order to 'learn' the meanings of words from their embedded contexts. Using word2vec and my generated x/y coordinates, I was able to map vectors onto a semantic space, colored by POS tags:\n",
    "\n",
    "![WE1.png](WE1.png)\n",
    "\n",
    "When mapping words, clustering and closeness between points suggests similar meaning.\n",
    "\n",
    "### Sentiment Analysis \n",
    "\n",
    "Sentiment Analysis allows us to computationally observe \"opinion, sentiment and subjectivity in text\". Essentially, we can detect emotions and attitudes. However, Syuzhet sentiment analysis is unable to capture as much of the complexity in emotion. A sentiment score assigns \"positive\" or \"negative\" implications to terms, so I was able to plot the average sentiment of each of my novels:\n",
    "\n",
    "![SA1.png](SA1.png)\n",
    "\n",
    "My corpus was evenly split between averaged positive and negative sentiments by book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d86fe-0f86-4f66-b09f-36de39516109",
   "metadata": {},
   "source": [
    "## Interpretation. \n",
    "\n",
    "*Provide your interpretation of the results of exploration, and any conclusion if you are comfortable making them.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e65623-0ed5-4257-bd48-6b495e847cfc",
   "metadata": {},
   "source": [
    "By performing the four different text analytics methods we learned in class, I was able to computationally analyze the 19th-20th century scifi genre as well as compare the books within it. PCA gave me some interesting results to compare the books in my corpus, and it singled out George Tucker's *A Voyage to the Moon* (1827). *A Voyage to the Moon* is regarded as earlier proto-scifi and social satire, so it is understandable why it's range is so much more wider and differing than that of the other novels in my corpus. \n",
    "\n",
    "Given that the LDA model outputted Topic 8 (labor, class, men, nation, day, time, people, course, army) to have the largest term frequency and document weight, my results for topic analysis suggest that these are the subjects which visionary scifi authors were thinking most about for society's future. These terms are strongly instutional which makes sense. These authors were very astute to consider how society, particularly nationhood, would maintain its structure. Coming out of the Industrial Revolution, it also makes sense that conversations surrounding class and labor were on their minds. The word \"army\" in this topic was also striking and suggests how power and nationhood is maintained by power and violence.\n",
    "\n",
    "The Word2vec was a little more difficult to apply meaningful interpretations to for the sorts of questions I wanted to ask of my corpus, but it was interesting to see how words were clustered together. I could see elements of the body ('hands', 'eyes', etc) clustered together. One fascinating extention of Word Embedding is to capture analogies between words which would be a good idea for future work.\n",
    "\n",
    "Lastly, I wondered why certain books were overall positive or negative in sentiment, and tie that to the synopsis of each book. According to Goodreads, In *A Voyage to the Moon* the narrator is shipwrecked off the coast of Burma, where he befriends a brahmin, who starts telling him of his trips to the moon using a space ship of his own invention. I can already imagine the whimsy albeit absurdity of this book, so it makes sense to me this one would have a positive overall tone. Next, Encyclopedia Brittanica describes *Looking Backward* as \"an indictment of the capitalistic system and an imaginative picturing of a utopia achieved by a collectivist society in the year 2000\". Achieving utopia is enough of a description to call this book a fairly optimistic one in the genre of scifi. Lastly for the positive books, *The Brick Moon* describes a retelling of a group of college friends who engineer the idea of a mechanism for more accurately telling longitude. Although there are setback and failures for these friends, the book frames their experience with joy, curiousity and fulfillment in their work. I understand why it's slightly less positive than the other two positive books.\n",
    "\n",
    "For the negative books, *The Iron Heel* tells the story of a \"20th century America that falls to a dictatorial oligarchy\". This tied into our most important topic within our topic model: the words \"nation\" and \"army\" specifically in mind. *The Variable Man* discusses a similar idea of societal downfall. In the novel, \"the human race has achieved space travel and begun to spread out from Earth, but is limited by an old and corrupt Centauran Empire, ruled from Proxima Centauri.\" This narrative is very similar to *the Iron Heel* in the sense of highlight corruption in governance and the crumbling of humankind as a result. Lastly with the most negative weighted sentiment, *Youth* narrates two boys who find two strange animals and capture them in attempt to put them in a circus; meanwhile, two professionals are tasked with the decision to open up their world to interstellar trade. *Youth* was the one book that surprised me for a negative sentiment. My guess is that some of the animals not surviving/succeeding in the book, coupled by the anxiety of the working professionals about the consequences of their decisions, gave this book a negative sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129095a-1fdf-4175-a59b-70b2d14316aa",
   "metadata": {},
   "source": [
    "## Resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7934f-7f22-4fc1-9b8c-ae37010524b2",
   "metadata": {},
   "source": [
    "Dropbox link: https://www.dropbox.com/scl/fo/vo4jx7bw8d0ybyiy9bhdm/h?dl=0&rlkey=u49a6y1hb67ic1nnypw554ahk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b8a4e-170a-469c-b011-ed2c19e42d85",
   "metadata": {},
   "source": [
    "All code resourced from class labs and lecture: https://github.com/ontoligent/DS5001-2023-01-R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce348e-ae11-41de-a3e9-1b37988eea24",
   "metadata": {},
   "source": [
    "“Science Fiction.” Encyclopædia Britannica, Encyclopædia Britannica, Inc., 21 Apr. 2023, https://www.britannica.com/art/science-fiction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
